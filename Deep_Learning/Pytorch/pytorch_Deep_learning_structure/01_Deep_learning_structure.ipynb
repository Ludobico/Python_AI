{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 구조\n",
    "딥러닝이란 `여러 층을 가진 인공 신경망`을 사용하여 학습을 수행하는 것이라고 했습니다. 그렇다면 각각의 층은 어떻게 구성되었고, 또 각 층의 역할은 무엇인지 알아보겠습니다. 그 전에 딥러닝에서 사용되는 용어부터 살펴보겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 용어\n",
    "딥러닝을 위한 용어들부터 알아봅시다. 딥러닝은 다음 그림과 같이 `입력층`, `출력층`,과 두 개 이상의 `은닉층`으로 구성되어 있습니다. 또한, 입력 신호를 전달하기 위해 다양한 함수도 사용하고 있는데, 신경망을 이루는 구성 요소에 대해 하나씩 살펴보겠습니다.\n",
    "\n",
    "![](../pytorch_wikidocs/Static/141.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝을 구성하는 요소들을 정리하면 다음과 같습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 층\n",
    "\n",
    "`입력층(input_layer)` 데이터를 받아들이는 층\n",
    "\n",
    "`은닉층(hidden layer)` 모든 입력 노드부터 입력 값을 받아 가중합을 계산하고, 이 값을 활성화 함수에 적용하여 출력층에 전달하는 층\n",
    "\n",
    "`출력층(output layer)` 신경망의 최종 결괏값이 포함된 층"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치\n",
    "\n",
    "`가중치(weight)` 노드와 노드 간 연결 강도"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바이어스\n",
    "\n",
    "`편향(bias)` 가중합에 더해 주는 상수로, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할을 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중합\n",
    "\n",
    "`가중합(weighted sum)`, `전달 함수` 가중치와 신호의 곱을 합한 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수\n",
    "\n",
    "`활성화 함수(activation function)` 신호를 입력받아 이를 적절히 처리하여 출력해 주는 함수\n",
    "\n",
    "`손실 함수(loss function)` 가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력층, 은닉층, 출력층은 위의 정의를 참고하면 되고, 나머지 용어는 하나씩 좀 더 자세히 살펴보겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치\n",
    "\n",
    "가중치는 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소입니다. 예를 들어 다음 그림에서 `w1` 값이 0 혹은 0과 가까운 0.0001 이라면, `x1`이 아무리 큰 값이라도 x1 x w1 값은 0이거나 0에 가까운 값이 됩니다. 이와 같이 입력 값의 연산 결과를 조정하는 역할을 하는 것이 가중치입니다.\n",
    "\n",
    "![](../pytorch_wikidocs/Static/142.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중합 또는 전달 함수\n",
    "\n",
    "가중합은 전달 함수라고도 합니다. 각 노드에서 들어오는 신호에 가중치를 곱해서 다음 노드로 전달되는데, 이 값들을 모두 더한 합계를 가중합이라고 합니다. 또한, 노드의 가중합이 계산되면 이 가중합을 활성화 함수로 보내기 때문에 전달 함수(transfer function)이라고 합니다.\n",
    "\n",
    "![](../pytorch_wikidocs/Static/143.jpg)  \n",
    "\n",
    "가중합을 구하는 공식은 다음과 같습니다.\n",
    "\n",
    "![](../pytorch_wikidocs/Static/fn-8.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수\n",
    "\n",
    "다음으로 함수들에 대해 알아보겠습니다. 먼저 활성화 함수는 전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수힙니다.  \n",
    "활성화 함수로는 `시그모이드(sigmoid)`, `하이퍼볼릭 탄젠트(hyperbolic tangent)`, `렐루(ReLU)` 함수 등이 있습니다. 시그모이드 함수부터 하나씩 살펴보겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시그모이드 함수\n",
    "\n",
    "시그모이드 함수는 선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형해 줍니다. 주로 로지스틱 회귀와 같은 `분류 문제를 확률적으로 표현`하는 데 사용됩니다. 과거에는 인기가 많았으니, 딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는 `기울기 소멸 문제(vainshion gradient problem)`가 발생하여 딥러닝 모델에서는 잘 사용하지 않습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼볼릭 탄젠트 함수\n",
    "\n",
    "하이퍼볼릭 탄젠트 함수는 선형 함수의 결과를 -1 ~ 1 사이에서 비선형 형태로 변형해 줍니다. 시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만, 기울기 소멸 문제는 여전히 발생합니다.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 렐루 함수\n",
    "최근 활발히 사용되는 렐루 함수는 입력(x)이 음수일 때는 0을 출력하고, 양수일 때는 x를 출력합니다. 경사 하강법(gradient discent)에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는 장점이 있습니다. 렐루 함수는 일반적으로 `은닉층`에서 사용되며, 하이퍼볼릭 탄젠트 함수 대비 학습 속도가 6배 빠릅니다. 문제는 음수 값을 입력받으면 항상 0을 출력하기 때문에 학습 능력이 감소하는데, 이를 해결하려고 리키 렐루(Leaky ReLU) 함수 등을 사용합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리키 렐루 함수\n",
    "리키 렐루 함수는 입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환합니다. 이렇게 하면 입력값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소프트맥스 함수\n",
    "\n",
    "소프트맥스 함수는 입력 값을 0~1 사이에 출력되도록 정규화하여 `출력 값들의 총합이 항상 1이 되도록` 합니다. 소프트맥스 함수는 보통 딥러닝에서 `출력 노드의 활성화 함수`로 많이 사용됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "\n",
    "경사 하강법은 학습률(lr)과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트 하는 방법입니다. 즉, 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법이라고 할 수 있습니다. 이때 `오차를 구하는 방법`이 손실 함수입니다.\n",
    "\n",
    "즉, 손실 함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표라고 할 수 있습니다. 이 값이 클수록 많이 틀렸다는 의미이고, 이 값이 0에 가까우면 완벽하게 추정할 수 있다는 의미입니다. 대표적인 손실 함수로는 `평균 제곱 오차(MSE)`와 `크로스 엔트로피 오차(CEE)`가 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차\n",
    "\n",
    "실제 값과 예측 값의 차이(error)를 제곱하여 평균을 낸 것이 평균 제곱 오차입니다. 실제값과 예측 값의 차이가 클수록 평균 제곱 오차의 값도 커진다는 것은 반대로 생각하면 이 값이 작을수록 예측력이 좋다는 것을 의미합니다. 평균 제곱 오차는 `회귀에서 손실 함수로` 주로 사용됩니다.\n",
    "\n",
    "파이토치에서는 다음과 같이 사용됩니다.\n",
    "```py\n",
    "import torch\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "y_pred = model(x)\n",
    "loss = loss_fn(y_pred, y)\n",
    "```\n",
    "\n",
    "#### 크로스 엔트로피 오차\n",
    "\n",
    "크로스 엔트로피 오차는 `분류 문제`에서 `원-핫 인코딩`했을 때만 사용할 수 있는 오차 계산법입니다.\n",
    "\n",
    "일반적으로 분류 문제에서는 데이터의 출력을 0과 1로 구분하기 위해 시그모이드 함수를 사용하는데, 시그모이드 함수에 포함된 자연 상수 e 때문에 평균 제곱 오차를 적용하면 매끄럽지 못한 그래프가 출력됩니다. 따라서 크로스 엔트로피 손실 함수를 사용하는데, 이 손실 함수를 적용할 경우 경사 하강법 과정에서 학습이 지역 최소점에서 멈출 수 있습니다. 이것을 방지하고자 자연 상수 e에 반대되는 자연 로그를 모델의 출력 값에 취합니다.\n",
    "\n",
    "파이토치에서는 다음과 같이 사용됩니다.\n",
    "\n",
    "```py\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(5, 6, requires_grad=True) ## torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성\n",
    "target = torch.empty(3, dtype=torch.long).random_(5) ## torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "969136afa1ab1e40cee0417c89a6b891bc5152d071e81b6238bc15b76e770f08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
