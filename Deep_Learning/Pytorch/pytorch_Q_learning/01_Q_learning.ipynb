{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 큐-러닝\n",
    "\n",
    "큐-러닝(Q-learning)은 모델 없이 학습하는 강화 학습 기법 중 하나입니다. 큐-러닝은 마르코프 결정 과정에서 최적의 정책을 찾는 데 사용됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큐-러닝은 에이전트가 주어진 상태에서 행동을 취했을 경우 받을 수 있는 보상의 기댓값을 예측하는 `큐-함수(Q-function)`를 사용하여 최적화된 정책을 학습하는 강화 학습 기법입니다. 즉, 큐-러닝은 여러 실험(episode)을 반복하여 최적의 정책을 학습합니다.\n",
    "\n",
    "![](../pytorch_wikidocs/Static/656.jpg)\n",
    "\n",
    "매 실험에서 각 상태마다 행동을 취하는데, 이때 행동은 랜덤한 선택을 하게 합니다. 그 이유는 가 보지 않을 곳을 탐험하면서 새로운 좋은 경로를 찾으려고 하기 때문이죠. 이렇듯 새로운 길을 탐험하는 것을 말 그대로 탐험이라 정의하고, `그리디` 방법을 이용하여 수행합니다. 0~1 사이로 랜덤하게 난수를 추출해서 그 값이 특정 임계치(threshold)보다 낮으면 랜덤하게 행동을 취합니다. 그리고 임계치는 실험이 반복되면서(학습이 진행되면서) 점점 낮은 값을 갖습니다. 따라서 학습이 수만 번 진행되면 임계치 값은 거의 0에 수렴하고, 행동을 취하고, 보상을 받고, 다음 상태를 받아 현재 상태와 행동에 대한 큐 값을 업데이트하는 과정을 무수히 반복합니다.\n",
    "\n",
    "예를 들어 이것을 코드로 구현하면 다음과 같습니다.\n",
    "\n",
    "```py\n",
    "import numpy as np\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1)) # 행동중 가장 보상(r)이 큰 행동을 고르고, 랜덤 노이즈 방식으로 활용과 탐험 구현\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action) # 해당 행동을 했을 때 환경이 변하고, 새로운 상태(state),보상(reward),완료(done) 여부를 반환\n",
    "\n",
    "    Q[state, action] = reward + dis * np.max(Q[new_state, :]) # Q = R + Q\n",
    "\n",
    "    rAll += reward\n",
    "    state = new_state\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "앞서 탐험이라는 단어를 정의해 보았습니다. 경험으로 학습하는 강화 학습에서 최단 시간에 주어진 환경의 모든 상태를 관찰하고, 이를 기반으로 보상을 최대화할 수 있는 행동을 수행하려면 `활용(exploitation)` 과 `탐험(exploration)` 사이의 균형이 필요합니다.\n",
    "\n",
    "그럼 활용이란 무엇일까요? 활용이란 `현재까지 경험 중 현 상태에서 가장 최대의 보상을 받을 수 있는 행동`을 하는 것입니다. 이러한 다양한 경험을 쌓기 위한 새로운 시도를 탐험이라고 합니다. 탐험을 통해 얻는 경험이 늘 최상의 결과를 얻는 것은 아니기 때문에 시간과 자원에 대한 낭비가 발생합니다. 즉, 풍부한 경험이 있어야만 더 좋은 선택을 할 수 있지만, 경험을 쌓기 위한 새로운 시도들은 시간과 자원이 낭비되기 때문에 이 둘 사이의 균형이 필요합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활용과 탐험을 반복하는 큐-러닝의 학습 절차는 다음과 같습니다.\n",
    "\n",
    "1. `초기화` 큐-테이블(Q-table)에 있는 모든 큐 값을 0으로 초기화합니다.  \n",
    "    예를 들어 0으로 초기화하는 코드는 다음과 같습니다.   \n",
    "    ```py  \n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    ```\n",
    "\n",
    "2. 행동 `a`를 선택하고 실행합니다.\n",
    "\n",
    "3. 보상 `r`과 다음 상태 `s'`를 관찰합니다.\n",
    "\n",
    "4. 상태 `s'`에서 가능한 모든 행동에 대해 가장 높은 큐 값을 갖는 행동인 `a'`를 선택합니다.\n",
    "\n",
    "5. 다음 공식을 이용하여 상태에 대한 큐 값을 업데이트합니다.   \n",
    "    ![](../pytorch_wikidocs/Static/fn2-48.jpg)  \n",
    "\n",
    "6. 종료 상태에 도달할 때까지 2~5를 반복합니다.\n",
    "\n",
    "하지만 이러한 큐-러닝은 실제로 실행해 보면 다음 이유로 잘 동작하지 않는 경우가 빈번합니다.\n",
    "\n",
    "* 에이전트가 취할 수 있는 상태 개수가 많은 경우 큐-테이블 구축에 한계가 있습니다.\n",
    "* 데이터 간 상관관계로 학습이 어렵습니다.\n",
    "\n",
    "이와 같은 이유로 큐-러닝은 잘 동작하지 않습니다. 이러한 큐-러닝의 단점을 보완하고자 큐러닝 기반의 `DQN(Deep Q Network)`이 출현했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "969136afa1ab1e40cee0417c89a6b891bc5152d071e81b6238bc15b76e770f08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
