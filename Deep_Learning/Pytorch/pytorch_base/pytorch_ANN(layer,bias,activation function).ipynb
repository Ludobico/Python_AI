{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN : 인공신경망\n",
    "* 사람의 두뇌와 비슷한 방식으로 정보를 처리하기 위한 구조\n",
    "    * 입력층(Input layer)\n",
    "        * 입력 데이터를 받기 위한 layer, 사람으로 치면 자극을 입력받는 감각기관\n",
    "    * 은닉층(Hidden layer)\n",
    "        * 입력된 데이터를 해석하기 위한 layer, 주로 context 데이터를 생성하며 사람으로 치면 입력된 자극을 해석\n",
    "    * 출력층(Output layer)\n",
    "        * 입력 데이터에 대한 출력 데이터를 도출"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 노드마다 수학연산을 실행하여 모델을 학습하며, 가장 핵심 변수는 아래와 같다.\n",
    "`가중치(Wight)`,`편향(Bias)`,`활성함수(Activation function)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가중치(Weight)\n",
    "    * 입력 신호가 출력에 주는 영향을 계산하는 매개변수\n",
    "* 편향(Bias)\n",
    "    * 각 노드가 데이터에 얼마나 민감한지 알려줄 수 있는 매개변수로 실제 사용자가 부여하거나 랜덤으로 초기화 하는 값\n",
    "* 활성 함수(Activation function)\n",
    "    * `Hidden layer`내 계산된 결과값을 activation function을 거침으로써 activate 여부를 확인하고 실제 ANN의 결과값을 산출, 즉 현재 노드에서 계산된 값을 다음 뉴런으로 보내줄지 말지 결정하는 역할을 한다 봐도 좋다.\n",
    "* 역전파(Backpropagation)\n",
    "    * 내가 뽑고자 하는 target값과 실제 모델이 예측한 output(prediction)이 얼마나 차이 나는지 오차를 구한 후, 이전 Layer로 해당 오차를 전파해가며, 각 노드가 갖고 있는 변수를 갱신하는 알고리즘"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "* 대다수 비선형 함수를 사용하여 구현하며, 크게 `Sigmoid`,`Tanh`,`ReLU`함수가 대표적이다. 이 외에도 `Leaky ReLU`,`PReLU`,`ELU`,`Maxout` 등 다양한 함수들이 존재하며, Task 종류에 따라 activation function의 종류도 큰 영향을 미친다.\n",
    "\n",
    "### sigmoid\n",
    "* sigmoid는 출력 값(y)를 0~1 사이의 값으로 제한시키며, 데이터의 평균을 0.5로 수렴하도록 한다.\n",
    "* Logistic Regresstion(로지스틱 회귀)같은 `분류(classification)`문제에 많이 사용되지만, 딥러닝 모델의 깊이가 깊어질수록 gradient descent 과정에서 기울기값이 사라지는 `Vanishing Gradient`현상이 발생하며, 얉은 모델에만 사용됨.\n",
    "* 대부분 sigmoid를 쓰면 성능이 낮기 때문에 비추천하지만 `binary classification(이중 분류)`문제에는 sigmoid를 사용\n",
    "\n",
    "### Tanh\n",
    "* 출력 값(y)를 -1~1 사이의 비선형 형태로 변경하기 위함.\n",
    "* sigmoid의 단점을 보완하기 위해 나와서, sigmoid 보다는 우수한 성능이지만 여전히 Vanishing Gradient 문제는 남아있음\n",
    "\n",
    "### ReLU\n",
    "* Vanishing Gradient 문제를 해결하기 위한 함수로 x값이 음수일 경우 0을 출력하게끔 하는 함수\n",
    "* 다른 함수에 비해 경사 하강에 영향을 주지 않기 때문에 학습이 빠르다\n",
    "* 일반적으로 Hidden Layer에 사용되는 함수지만, 음수 값을 받는 경우 항상 0을 출력하기 때문에 데이터 종류에 따라 훈련이 덜 될수도 있다.\n",
    "\n",
    "### SoftMax\n",
    "* 실제로 classification 관련 Task에서 가장 많이 쓰이는 함수이며, k개의 클래스에 대한 확률 분포, 즉 Logistic regression이라 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc4cb84d376c55cbcefbb2577a30e93afba84ecdcb02495984b572f007c87af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
