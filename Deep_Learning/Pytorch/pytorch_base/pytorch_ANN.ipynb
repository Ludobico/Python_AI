{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "텐서(Tensor) : 파이토치에서 다양한 수식을 계산하는데 사용하는 기본적 자료구조\n",
    "\n",
    "* 차원(랭크) 0 : 숫자가 1개(scalar)\n",
    "* 차원(랭크) 1 : 일렬로 숫자가 나열(vector)\n",
    "* 차원(랭크) 2 : 숫자를 가로, 세로로 나열(matrix)\n",
    "* 차원(랭크) n : n차원(n-tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUDOBICO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "size torch.Size([3, 3])\n",
      "shape torch.Size([3, 3])\n",
      "rank 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 3x3 텐서 선언\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(x)\n",
    "\n",
    "print('size', x.size())\n",
    "print('shape', x.shape)\n",
    "print('rank', x.ndim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 연산 과정중 특정 Attention Layer 또는 데이터 간의 합성곱을 수행할 때 각 Tensor가 서로 다른 형태일 경우, 다음과 같이 `unsqueeze`를 통해 차원을 늘린 뒤 곱하고, 이후에 `squeeze`로 불필요한것을 다시 삭제한 뒤 FC에 넣는 형태로 많이 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "size: torch.Size([1, 3, 3])\n",
      "shape: torch.Size([1, 3, 3])\n",
      "rank: 3\n"
     ]
    }
   ],
   "source": [
    "# 텐서 차원 늘리기 (dim = 0번째 자리에 차원 추가)\n",
    "x = torch.unsqueeze(x, 0)\n",
    "print(x)\n",
    "print('size:', x.size())\n",
    "print('shape:',x.shape)\n",
    "print('rank:',x.ndim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 외에도 `view()`함수를 통해 직접 모양을 변경할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "size: torch.Size([9])\n",
      "shape: torch.Size([9])\n",
      "rank: 1\n"
     ]
    }
   ],
   "source": [
    "x = x.view(9)\n",
    "print(x)\n",
    "print('size:', x.size())\n",
    "print('shape:',x.shape)\n",
    "print('rank:',x.ndim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서간 행렬곱\n",
    "* a의 열 수와 b의 행 수는 동일해야 한다.\n",
    "* 행렬곱 a x b의 결과 행렬의 행 개수는 a와 같고, 열의 개수는 b와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w size: torch.Size([5, 3])\n",
      "x size: torch.Size([3, 2])\n",
      "w: tensor([[-0.4367,  1.5152,  0.1166],\n",
      "        [-1.3173,  0.6072, -0.1277],\n",
      "        [-0.4242,  0.2385,  1.3095],\n",
      "        [ 0.8849,  0.8043, -0.8749],\n",
      "        [-0.6930, -0.4887,  1.8551]])\n",
      "x: tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.randn(5,3, dtype=torch.float)\n",
    "x = torch.tensor([[1.0, 2.0],[3.0, 4.0],[5.0, 6.0]])\n",
    "\n",
    "print('w size:', w.size())\n",
    "print('x size:', x.size())\n",
    "print('w:',w)\n",
    "print('x:',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b size: torch.Size([5, 2])\n",
      "b: tensor([[-0.6464,  0.8179],\n",
      "        [-1.4951, -0.1950],\n",
      "        [-0.3608,  0.3899],\n",
      "        [-0.4352,  0.8767],\n",
      "        [ 0.3300,  0.3973]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randn(5,2,dtype=torch.float)\n",
    "print('b size:',b.size())\n",
    "print('b:',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wx size: torch.Size([5, 2])\n",
      "ws: tensor([[ 4.6920,  5.8871],\n",
      "        [-0.1340, -0.9717],\n",
      "        [ 6.8385,  7.9622],\n",
      "        [-1.0770, -0.2628],\n",
      "        [ 7.1166,  7.7901]])\n"
     ]
    }
   ],
   "source": [
    "wx = torch.mm(w,x)\n",
    "print('wx size:',wx.size())\n",
    "print('ws:',wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result size: torch.Size([5, 2])\n",
      "result: tensor([[ 4.0456,  6.7050],\n",
      "        [-1.6291, -1.1667],\n",
      "        [ 6.4777,  8.3521],\n",
      "        [-1.5122,  0.6139],\n",
      "        [ 7.4466,  8.1874]])\n"
     ]
    }
   ],
   "source": [
    "result = wx+b\n",
    "print('result size:',result.size())\n",
    "print('result:',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc4cb84d376c55cbcefbb2577a30e93afba84ecdcb02495984b572f007c87af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
