{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순전파와 역전파\n",
    "`신경망(Neural Network)`은 어떤 입력 데이터에 대해 실행되는 중첩된 함수들의 집합체입니다. 신경망을 아래 2단계를 거쳐 학습됩니다.\n",
    "* 순전파(Forward Propagation)\n",
    "* 역전파(Backward Propagation)\n",
    "\n",
    "![](./Static/img1.png)\n",
    "\n",
    "### 순전파(Forward Propagation)\n",
    "순전파 단계에서, 신경망은 정답을 맞추기 위해 최선의 추측(best guess)을 합니다. 이렇게 추측을 하기 위해서 입력 데이터를 각 함수들에서 실행합니다.\n",
    "\n",
    "### 역전파(Backward Propagation)\n",
    "역전파 단계에서, 신경망은 추측한 값에서 발생한 error에 비례하여 파라미터들을 적절히 업데이트합니다. 출력(output)으로부터 역방향으로 이동하면서 오류에 대한 함수들의 매개변수들이 미분값(gradient)를 수집하고, `경사하강법(gradient descent)`을 사용하여 매개변수들을 최적화 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴네트워크 학습 알고리즘\n",
    "\n",
    "* 모든 가중치 w를 임의로 생성\n",
    "    * Forward Propagation\n",
    "\n",
    "* 입력변수 값과 입력층과 은닉층 사이의 w값을 이용하여 은닉노드의 값을 계산\n",
    "    * 선형결합 후 activation한 값\n",
    "\n",
    "* 은닉노드의 값과 은닉층과 출력층 사이의 w값을 이용하여 출력노드의 값을 계산\n",
    "    * 선형결합 후 activation한 값\n",
    "    * Backward Propagation\n",
    "\n",
    "* 계산된 출력노드의 값과 실제 출력변수의 값의 차이를 줄일 수 있도록 은닉층과 출력층 사이의 w값을 업데이트\n",
    "\n",
    "* 에러가 충분히 줄어들 때까지 반복"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd 개념\n",
    "`pyTorch`를 이용해 코드를 작성할때 이러한 역전파를 통해 파라미터를 업데이트하는 방법은 바로 `Autograd`입니다. 차근차근 코드를 통해 알아보도록 합시다. Autograd에 대해 알아보기 위해 간단한 MLP(Multi-Layer Perceptron)을 예시로 들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUDOBICO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "batch_size는 딥러닝 모델이 `파라미터를 업데이트할 때 계산되는 데이터 묶음의 갯수`입니다. 앞에서 Neural Network가 순전파와 역전파를 수행하면서 파라미터를 업데이트를 한다고 소개 드렸는데, 이러한 업데이트를 수행하는 데 사용되는 데이터 단위(갯수)가 되는 것이 바로 바로 batch_size입니다. 아래 예시에서 batch_size를 32로 지정해줬는데, 이는 코드 작성자 마음대로 정해주는 하이퍼파라미터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Static/batch_size.png)\n",
    "\n",
    "* `INPUT_SIZE`는 딥러닝 모델의 입력값의 크기이며, 입력층의 노드 수를 의미합니다.\n",
    "\n",
    "* `HIDDEN_SIZE`는 입력값에 다수의 파라미터를 사용하여 계산되는 값의 개수로, 은닉 층의 노드 수를 의미합니다.\n",
    "\n",
    "* `OUTPUT_SIZE`는 은닉값에 다수의 파라미터를 사용하여 계산되는 결과값의 개수로, 출력 층의 노드 수를 의미합니다.\n",
    "\n",
    "* `LEARNING_RATE`는 Gradient를 업데이트할 때 곱해주는 0과 1 사이에 존재하는 값입니다. 좀 더 느리지만 섬세하고 촘촘히 업데이트를 원하면 작은 rate를, 좀 더 빠르게 업데이트를 원하면 큰 rate를 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 지정\n",
    "INPUT_SIZE = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 2\n",
    "LEARNING_RATE = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 x,y, weight 정의\n",
    "# x : input 값 >> (32, 1000)\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, INPUT_SIZE, device=device, dtype=torch.float, requires_grad= False)\n",
    "\n",
    "# y : output 값 >> (32, 2)\n",
    "y = torch.randn(BATCH_SIZE, OUTPUT_SIZE, device=device,\n",
    "                dtype=torch.float, requires_grad=False)\n",
    "\n",
    "# w1 : input -> hidden >> (1000, 100)\n",
    "w1 = torch.randn(INPUT_SIZE, HIDDEN_SIZE, device=device,\n",
    "                 dtype=torch.float, requires_grad=False)\n",
    "\n",
    "# w2 : hidden -> output >> (100, 2)\n",
    "w2 = torch.randn(HIDDEN_SIZE, OUTPUT_SIZE, device=device,\n",
    "                 dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# 실험환경을 위해 다음과 같이 임의의 값 input(x), output(y), weight(w1,w2)\n",
    "# 를 정의해 줍니다.\n",
    "\n",
    "# 이때, requires_grad=True는 모든 연산들을 추적해야 한다고 알려줍니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (iteration = 500)\n",
    "\n",
    "* 본 포스트는 Autograd를 확인해보는 포스트이므로, 단순하게 for문을 이용하여 500번의 iteration을 수행하도록 코드를 작성하였습니다.\n",
    "\n",
    "* `torch.mm()` : mm은 matrix multiplication의 줄임말으로, `행렬의 곱셈`을 의미합니다.\n",
    "\n",
    "* `torch.nn.ReLU()` : ReLU함수, ReLU는 max(0,x)를 의미하는 함수인데, 0보다 작아지게 되면 0이 되고, 그 이상은 값을 유지한다는 특성을 가지고 있습니다.\n",
    "\n",
    "* `loss.backward()` : loss에 대해서 backward를 호출한 것으로, autograd는 각 파라미터 값에 대해 미분값을 계산하고 이를 각 텐서의 grad속성(attribute)에 저장합니다.\n",
    "\n",
    "* `with torch.no_grad()` : 미분값 계산을 사용하지 않도록 설정하는 컨텍스트-관리자(context-manager)입니다. 해당모드는 입력에 requires_grad = True 가 있어도 False로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19012\\89267434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# 가중치 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mw1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mw2\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 500 iteration\n",
    "for t in range(1, 501):\n",
    "    # 은닉값\n",
    "    hidden = torch.nn.functional.relu(x.mm(w1))\n",
    "    \n",
    "    # 예측값\n",
    "    y_pred = hidden.mm(w2)\n",
    "    \n",
    "    # 오차제곱합 계산\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    # iteration 100마다 기록\n",
    "    if t % 100 == 0:\n",
    "        print(t, 'th Iteration: ', sep= '')\n",
    "        print('>>>> Loss: ', loss.item())\n",
    "        \n",
    "    # Loss의 Gradient 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 해당 시점의 Gradient값을 고정\n",
    "    with torch.no_grad():\n",
    "        # 가중치 업데이트\n",
    "        w1 -= LEARNING_RATE * w1.grad\n",
    "        w2 -= LEARNING_RATE * w2.grad\n",
    "        \n",
    "        # 가중치 Gradient 초기화(0)\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc4cb84d376c55cbcefbb2577a30e93afba84ecdcb02495984b572f007c87af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
