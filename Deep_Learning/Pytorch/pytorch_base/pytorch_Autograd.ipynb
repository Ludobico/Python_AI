{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순전파와 역전파\n",
    "`신경망(Neural Network)`은 어떤 입력 데이터에 대해 실행되는 중첩된 함수들의 집합체입니다. 신경망을 아래 2단계를 거쳐 학습됩니다.\n",
    "* 순전파(Forward Propagation)\n",
    "* 역전파(Backward Propagation)\n",
    "\n",
    "![](./Static/img1.png)\n",
    "\n",
    "### 순전파(Forward Propagation)\n",
    "순전파 단계에서, 신경망은 정답을 맞추기 위해 최선의 추측(best guess)을 합니다. 이렇게 추측을 하기 위해서 입력 데이터를 각 함수들에서 실행합니다.\n",
    "\n",
    "### 역전파(Backward Propagation)\n",
    "역전파 단계에서, 신경망은 추측한 값에서 발생한 error에 비례하여 파라미터들을 적절히 업데이트합니다. 출력(output)으로부터 역방향으로 이동하면서 오류에 대한 함수들의 매개변수들이 미분값(gradient)를 수집하고, `경사하강법(gradient descent)`을 사용하여 매개변수들을 최적화 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴네트워크 학습 알고리즘\n",
    "\n",
    "* 모든 가중치 w를 임의로 생성\n",
    "    * Forward Propagation\n",
    "\n",
    "* 입력변수 값과 입력층과 은닉층 사이의 w값을 이용하여 은닉노드의 값을 계산\n",
    "    * 선형결합 후 activation한 값\n",
    "\n",
    "* 은닉노드의 값과 은닉층과 출력층 사이의 w값을 이용하여 출력노드의 값을 계산\n",
    "    * 선형결합 후 activation한 값\n",
    "    * Backward Propagation\n",
    "\n",
    "* 계산된 출력노드의 값과 실제 출력변수의 값의 차이를 줄일 수 있도록 은닉층과 출력층 사이의 w값을 업데이트\n",
    "\n",
    "* 에러가 충분히 줄어들 때까지 반복"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd 개념\n",
    "`pyTorch`를 이용해 코드를 작성할때 이러한 역전파를 통해 파라미터를 업데이트하는 방법은 바로 `Autograd`입니다. 차근차근 코드를 통해 알아보도록 합시다. Autograd에 대해 알아보기 위해 간단한 MLP(Multi-Layer Perceptron)을 예시로 들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUDOBICO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "batch_size는 딥러닝 모델이 `파라미터를 업데이트할 때 계산되는 데이터 묶음의 갯수`입니다. 앞에서 Neural Network가 순전파와 역전파를 수행하면서 파라미터를 업데이트를 한다고 소개 드렸는데, 이러한 업데이트를 수행하는 데 사용되는 데이터 단위(갯수)가 되는 것이 바로 바로 batch_size입니다. 아래 예시에서 batch_size를 32로 지정해줬는데, 이는 코드 작성자 마음대로 정해주는 하이퍼파라미터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Static/batch_size.png)\n",
    "\n",
    "* `INPUT_SIZE`는 딥러닝 모델의 입력값의 크기이며, 입력층의 노드 수를 의미합니다.\n",
    "\n",
    "* `HIDDEN_SIZE`는 입력값에 다수의 파라미터를 사용하여 계산되는 값의 개수로, 은닉 층의 노드 수를 의미합니다.\n",
    "\n",
    "* `OUTPUT_SIZE`는 은닉값에 다수의 파라미터를 사용하여 계산되는 결과값의 개수로, 출력 층의 노드 수를 의미합니다.\n",
    "\n",
    "* `LEARNING_RATE`는 Gradient를 업데이트할 때 곱해주는 0과 1 사이에 존재하는 값입니다. 좀 더 느리지만 섬세하고 촘촘히 업데이트를 원하면 작은 rate를, 좀 더 빠르게 업데이트를 원하면 큰 rate를 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 지정\n",
    "INPUT_SIZE = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 2\n",
    "LEARNING_RATE = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 x,y, weight 정의\n",
    "# x : input 값 >> (32, 1000)\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, INPUT_SIZE, device=device,\n",
    "                dtype=torch.float).requires_grad_()\n",
    "\n",
    "# y : output 값 >> (32, 2)\n",
    "y = torch.randn(BATCH_SIZE, OUTPUT_SIZE, device=device,\n",
    "                dtype=torch.float).requires_grad_()\n",
    "\n",
    "# w1 : input -> hidden >> (1000, 100)\n",
    "w1 = torch.randn(INPUT_SIZE, HIDDEN_SIZE, device=device,\n",
    "                 dtype=torch.float).requires_grad_()\n",
    "\n",
    "# w2 : hidden -> output >> (100, 2)\n",
    "w2 = torch.randn(HIDDEN_SIZE, OUTPUT_SIZE, device=device,\n",
    "                 dtype=torch.float).requires_grad_()\n",
    "\n",
    "# 실험환경을 위해 다음과 같이 임의의 값 input(x), output(y), weight(w1,w2)\n",
    "# 를 정의해 줍니다.\n",
    "\n",
    "# 이때, requires_grad=True는 모든 연산들을 추적해야 한다고 알려줍니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (iteration = 500)\n",
    "\n",
    "* 본 포스트는 Autograd를 확인해보는 포스트이므로, 단순하게 for문을 이용하여 500번의 iteration을 수행하도록 코드를 작성하였습니다.\n",
    "\n",
    "* `torch.mm()` : mm은 matrix multiplication의 줄임말으로, `행렬의 곱셈`을 의미합니다.\n",
    "\n",
    "* `torch.nn.ReLU()` : ReLU함수, ReLU는 max(0,x)를 의미하는 함수인데, 0보다 작아지게 되면 0이 되고, 그 이상은 값을 유지한다는 특성을 가지고 있습니다.\n",
    "\n",
    "* `loss.backward()` : loss에 대해서 backward를 호출한 것으로, autograd는 각 파라미터 값에 대해 미분값을 계산하고 이를 각 텐서의 grad속성(attribute)에 저장합니다.\n",
    "\n",
    "* `with torch.no_grad()` : 미분값 계산을 사용하지 않도록 설정하는 컨텍스트-관리자(context-manager)입니다. 해당모드는 입력에 requires_grad = True 가 있어도 False로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th Iteration: \n",
      ">>>> Loss:  3.5938907405608234e-09\n",
      "200th Iteration: \n",
      ">>>> Loss:  3.728950481729498e-09\n",
      "300th Iteration: \n",
      ">>>> Loss:  2.8433300158781094e-09\n",
      "400th Iteration: \n",
      ">>>> Loss:  2.824742217910625e-09\n",
      "500th Iteration: \n",
      ">>>> Loss:  2.474245919259488e-09\n",
      "600th Iteration: \n",
      ">>>> Loss:  2.4942548026984923e-09\n",
      "700th Iteration: \n",
      ">>>> Loss:  2.793649089838368e-09\n",
      "800th Iteration: \n",
      ">>>> Loss:  3.631009271032326e-09\n",
      "900th Iteration: \n",
      ">>>> Loss:  3.7303715672010185e-09\n",
      "1000th Iteration: \n",
      ">>>> Loss:  3.968147588295778e-09\n",
      "1100th Iteration: \n",
      ">>>> Loss:  3.3321765346983057e-09\n",
      "1200th Iteration: \n",
      ">>>> Loss:  2.9891620290101173e-09\n",
      "1300th Iteration: \n",
      ">>>> Loss:  3.324481134825419e-09\n",
      "1400th Iteration: \n",
      ">>>> Loss:  4.091185168419997e-09\n",
      "1500th Iteration: \n",
      ">>>> Loss:  3.6176228679352107e-09\n",
      "1600th Iteration: \n",
      ">>>> Loss:  3.6694638438916627e-09\n",
      "1700th Iteration: \n",
      ">>>> Loss:  3.1316897963762358e-09\n",
      "1800th Iteration: \n",
      ">>>> Loss:  3.1316897963762358e-09\n",
      "1900th Iteration: \n",
      ">>>> Loss:  3.2502651681198813e-09\n",
      "2000th Iteration: \n",
      ">>>> Loss:  2.8710058774805702e-09\n",
      "2100th Iteration: \n",
      ">>>> Loss:  2.8511675242981482e-09\n",
      "2200th Iteration: \n",
      ">>>> Loss:  3.454389663204438e-09\n",
      "2300th Iteration: \n",
      ">>>> Loss:  3.83092046973843e-09\n",
      "2400th Iteration: \n",
      ">>>> Loss:  2.884989358520329e-09\n",
      "2500th Iteration: \n",
      ">>>> Loss:  2.4043711466248396e-09\n",
      "2600th Iteration: \n",
      ">>>> Loss:  2.7877871122683473e-09\n",
      "2700th Iteration: \n",
      ">>>> Loss:  2.382372743525707e-09\n",
      "2800th Iteration: \n",
      ">>>> Loss:  2.767202689213377e-09\n",
      "2900th Iteration: \n",
      ">>>> Loss:  2.767202689213377e-09\n",
      "3000th Iteration: \n",
      ">>>> Loss:  2.4676947152357798e-09\n",
      "3100th Iteration: \n",
      ">>>> Loss:  2.4493342909437388e-09\n",
      "3200th Iteration: \n",
      ">>>> Loss:  3.874967013928199e-09\n",
      "3300th Iteration: \n",
      ">>>> Loss:  3.1413458501106106e-09\n",
      "3400th Iteration: \n",
      ">>>> Loss:  2.926484832244114e-09\n",
      "3500th Iteration: \n",
      ">>>> Loss:  3.100649514919951e-09\n",
      "3600th Iteration: \n",
      ">>>> Loss:  4.823111687812798e-09\n",
      "3700th Iteration: \n",
      ">>>> Loss:  5.22635890121137e-09\n",
      "3800th Iteration: \n",
      ">>>> Loss:  3.825630479070696e-09\n",
      "3900th Iteration: \n",
      ">>>> Loss:  3.906692747079887e-09\n",
      "4000th Iteration: \n",
      ">>>> Loss:  3.560065131580359e-09\n",
      "4100th Iteration: \n",
      ">>>> Loss:  2.9920788069404125e-09\n",
      "4200th Iteration: \n",
      ">>>> Loss:  2.828256073783564e-09\n",
      "4300th Iteration: \n",
      ">>>> Loss:  2.4312685198424333e-09\n",
      "4400th Iteration: \n",
      ">>>> Loss:  2.4710484769485674e-09\n",
      "4500th Iteration: \n",
      ">>>> Loss:  2.5897375355299346e-09\n",
      "4600th Iteration: \n",
      ">>>> Loss:  2.7407136560242407e-09\n",
      "4700th Iteration: \n",
      ">>>> Loss:  3.375142831885114e-09\n",
      "4800th Iteration: \n",
      ">>>> Loss:  2.3486717015686054e-09\n",
      "4900th Iteration: \n",
      ">>>> Loss:  2.7457724982582477e-09\n",
      "5000th Iteration: \n",
      ">>>> Loss:  2.5568323014368843e-09\n",
      "5100th Iteration: \n",
      ">>>> Loss:  2.6987063694861035e-09\n",
      "5200th Iteration: \n",
      ">>>> Loss:  3.2093945279143554e-09\n",
      "5300th Iteration: \n",
      ">>>> Loss:  3.107072821251222e-09\n",
      "5400th Iteration: \n",
      ">>>> Loss:  2.598722348423621e-09\n",
      "5500th Iteration: \n",
      ">>>> Loss:  1.8984114280584663e-09\n",
      "5600th Iteration: \n",
      ">>>> Loss:  2.9722970751322464e-09\n",
      "5700th Iteration: \n",
      ">>>> Loss:  3.4401184123566964e-09\n",
      "5800th Iteration: \n",
      ">>>> Loss:  2.882370786494448e-09\n",
      "5900th Iteration: \n",
      ">>>> Loss:  3.5869982539793455e-09\n",
      "6000th Iteration: \n",
      ">>>> Loss:  2.477983152004981e-09\n",
      "6100th Iteration: \n",
      ">>>> Loss:  2.907946772268133e-09\n",
      "6200th Iteration: \n",
      ">>>> Loss:  2.0399479883081995e-09\n",
      "6300th Iteration: \n",
      ">>>> Loss:  3.0145850260510088e-09\n",
      "6400th Iteration: \n",
      ">>>> Loss:  2.4780970608873076e-09\n",
      "6500th Iteration: \n",
      ">>>> Loss:  2.7819819781171873e-09\n",
      "6600th Iteration: \n",
      ">>>> Loss:  2.202868110146028e-09\n",
      "6700th Iteration: \n",
      ">>>> Loss:  2.9245521560028465e-09\n",
      "6800th Iteration: \n",
      ">>>> Loss:  4.03049060793137e-09\n",
      "6900th Iteration: \n",
      ">>>> Loss:  4.1268970463193e-09\n",
      "7000th Iteration: \n",
      ">>>> Loss:  3.236331203027021e-09\n",
      "7100th Iteration: \n",
      ">>>> Loss:  3.2633318269859046e-09\n",
      "7200th Iteration: \n",
      ">>>> Loss:  3.1907427811006528e-09\n",
      "7300th Iteration: \n",
      ">>>> Loss:  2.8719082667549856e-09\n",
      "7400th Iteration: \n",
      ">>>> Loss:  2.7650424172520616e-09\n",
      "7500th Iteration: \n",
      ">>>> Loss:  1.8786903144274447e-09\n",
      "7600th Iteration: \n",
      ">>>> Loss:  3.291476424749362e-09\n",
      "7700th Iteration: \n",
      ">>>> Loss:  3.849444318859696e-09\n",
      "7800th Iteration: \n",
      ">>>> Loss:  4.870799763523337e-09\n",
      "7900th Iteration: \n",
      ">>>> Loss:  4.642978446156576e-09\n",
      "8000th Iteration: \n",
      ">>>> Loss:  3.2813511907647808e-09\n",
      "8100th Iteration: \n",
      ">>>> Loss:  4.265197084407646e-09\n",
      "8200th Iteration: \n",
      ">>>> Loss:  3.720409758045662e-09\n",
      "8300th Iteration: \n",
      ">>>> Loss:  3.942099091602813e-09\n",
      "8400th Iteration: \n",
      ">>>> Loss:  3.5361589212357103e-09\n",
      "8500th Iteration: \n",
      ">>>> Loss:  3.5636711359643414e-09\n",
      "8600th Iteration: \n",
      ">>>> Loss:  4.516309992652623e-09\n",
      "8700th Iteration: \n",
      ">>>> Loss:  4.809074916067857e-09\n",
      "8800th Iteration: \n",
      ">>>> Loss:  3.755574518038429e-09\n",
      "8900th Iteration: \n",
      ">>>> Loss:  5.678701064937286e-09\n",
      "9000th Iteration: \n",
      ">>>> Loss:  3.515808977283541e-09\n",
      "9100th Iteration: \n",
      ">>>> Loss:  4.298542854996867e-09\n",
      "9200th Iteration: \n",
      ">>>> Loss:  3.4582834373964033e-09\n",
      "9300th Iteration: \n",
      ">>>> Loss:  3.5559546418539867e-09\n",
      "9400th Iteration: \n",
      ">>>> Loss:  5.416098680655068e-09\n",
      "9500th Iteration: \n",
      ">>>> Loss:  3.6459946173295066e-09\n",
      "9600th Iteration: \n",
      ">>>> Loss:  4.2800190058756016e-09\n",
      "9700th Iteration: \n",
      ">>>> Loss:  4.2070604777677545e-09\n",
      "9800th Iteration: \n",
      ">>>> Loss:  4.218201787864473e-09\n",
      "9900th Iteration: \n",
      ">>>> Loss:  4.458535762807969e-09\n",
      "10000th Iteration: \n",
      ">>>> Loss:  4.4608094995624015e-09\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 500 iteration\n",
    "for t in range(1, 10001):\n",
    "    # 은닉값\n",
    "    hidden = torch.nn.functional.relu(x.mm(w1))\n",
    "    \n",
    "    # 예측값\n",
    "    y_pred = hidden.mm(w2)\n",
    "    \n",
    "    # 오차제곱합 계산\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    # iteration 100마다 기록\n",
    "    if t % 100 == 0:\n",
    "        print(t, 'th Iteration: ', sep= '')\n",
    "        print('>>>> Loss: ', loss.item())\n",
    "        \n",
    "    # Loss의 Gradient 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 해당 시점의 Gradient값을 고정\n",
    "    with torch.no_grad():\n",
    "        # 가중치 업데이트\n",
    "        w1 -= LEARNING_RATE * w1.grad\n",
    "        w2 -= LEARNING_RATE * w2.grad\n",
    "        \n",
    "        # 가중치 Gradient 초기화(0)\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc4cb84d376c55cbcefbb2577a30e93afba84ecdcb02495984b572f007c87af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
