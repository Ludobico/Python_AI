{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 구축\n",
    "신경망은 데이터에 대한 작업을 수행하는 계층/모듈로 구성됩니다. `torch.nn` 네임스페이스는 자체 신경망을 구축하는데 필요한 모든 빌딩 블록을 제공합니다. Pytorch의 모든 모듈은 `nn.module` 하위 클래스 입니다. 신경망은 다른 모듈(계층)로 구성된 모듈 자체입니다. 이 중첩 구조를 통해 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있습니다.\n",
    "\n",
    "다음 섹션에서는 `FashionMNIST` 데이터 세트에서 이미지를 분류하기 위해 신경망을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUDOBICO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교육용 장치 가져오기\n",
    "가능할 경우 `GPU`와 같은 하드웨어 가속기에서 모델을 훈련할 수 있기를 원합니다. `torch.cuda`를 사용할 수 있는지 확인하고 그렇지 않으면 `CPU`를 계속 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 레이어\n",
    "`FashionMNIST` 모델의 레이어를 분석해 봅시다. 이를 설명하기 위해 28*28 크기의 3개 이미지로 구성된 샘플 미니 배치를 가져와 네트워크를 통과할때 어떤 일이 발생하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6658, 0.8941, 0.8231,  ..., 0.9084, 0.6256, 0.7452],\n",
      "         [0.1455, 0.5973, 0.4038,  ..., 0.6074, 0.8778, 0.4367],\n",
      "         [0.3683, 0.2715, 0.9017,  ..., 0.6207, 0.8422, 0.0907],\n",
      "         ...,\n",
      "         [0.3339, 0.8541, 0.2306,  ..., 0.6898, 0.5091, 0.4648],\n",
      "         [0.1584, 0.3947, 0.7571,  ..., 0.5516, 0.0486, 0.4440],\n",
      "         [0.4284, 0.9136, 0.0492,  ..., 0.5379, 0.3886, 0.8503]],\n",
      "\n",
      "        [[0.9301, 0.7293, 0.5883,  ..., 0.7988, 0.2654, 0.9554],\n",
      "         [0.5976, 0.1272, 0.1164,  ..., 0.6664, 0.1527, 0.6922],\n",
      "         [0.2858, 0.0706, 0.5293,  ..., 0.8955, 0.3994, 0.3778],\n",
      "         ...,\n",
      "         [0.2582, 0.8723, 0.9906,  ..., 0.3431, 0.0838, 0.7146],\n",
      "         [0.3864, 0.9524, 0.7115,  ..., 0.4412, 0.8238, 0.7761],\n",
      "         [0.8152, 0.2624, 0.1337,  ..., 0.0736, 0.8826, 0.2660]],\n",
      "\n",
      "        [[0.7115, 0.3883, 0.1458,  ..., 0.1956, 0.8849, 0.4227],\n",
      "         [0.4751, 0.9977, 0.5479,  ..., 0.3873, 0.4514, 0.0686],\n",
      "         [0.7392, 0.9315, 0.5417,  ..., 0.8042, 0.8476, 0.4893],\n",
      "         ...,\n",
      "         [0.6308, 0.6675, 0.3920,  ..., 0.6526, 0.1607, 0.1737],\n",
      "         [0.2631, 0.6382, 0.0654,  ..., 0.6965, 0.1948, 0.1233],\n",
      "         [0.5624, 0.9136, 0.4376,  ..., 0.2337, 0.6981, 0.6244]]])\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image)\n",
    "print(input_image.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Flatten*\n",
    "`nn.Flatten` 레이어를 초기화하여 각 2D 28*28 이미지를 784 픽셀 값의 연속 배열로 변환합니다. (미니배치 차원(dim=0에서)이 유지됨)\n",
    "\n",
    "`Flatten`은 일반적으로 입력 데이터의 구조를 깊은 계층 구조에서 평탄한 구조로 바꾸는 것을 의미합니다. 이것은 주로 신경망의 입력층과 은닉층사이에서 수행됩니다. 예를 들어, 이미지 데이터의 경우 깊은 계층 구조가 일반적으로 각 계층에서 고차원의 출력을 생성하기 때문에 이를 평탄한 구조로 바꾸기 위해 사용할 수 있습니다. 이렇게 하면 일반적으로 입력 데이터의 구조차원 구조가 일어나기 때문에 신경망이 처리하기 어려운 고차원 데이터를 처리할 수 있는 저차원 데이터로 바꾸어 주기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7131, 0.3762, 0.1229,  ..., 0.6660, 0.6250, 0.0683],\n",
      "        [0.8084, 0.1210, 0.0769,  ..., 0.0334, 0.6827, 0.0660],\n",
      "        [0.9019, 0.1220, 0.4534,  ..., 0.8527, 0.1880, 0.0622]])\n",
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.linear\n",
    "`선형 계층`은 저장된 가중치와 편향을 사용하여 입력에 선형 변환을 적용하는 모듈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2474,  0.4527, -0.6020, -0.1155, -0.3309, -0.1676, -0.2441, -0.9399,\n",
      "         -0.6062, -0.2157,  0.5055,  0.6485, -0.1223, -0.1734,  0.2810,  0.0141,\n",
      "         -0.0927, -0.2233,  0.3967,  0.2691],\n",
      "        [ 0.3556,  0.5093, -0.4995, -0.2223, -0.0264, -0.4979, -0.0013, -0.4729,\n",
      "         -0.2557, -0.3703,  0.1600,  0.9676, -0.0381, -0.0076, -0.1474, -0.1116,\n",
      "         -0.1684,  0.2044,  0.5976,  0.1881],\n",
      "        [ 0.3988,  0.5375, -0.9643, -0.2940, -0.1489, -0.3664, -0.0280, -0.5056,\n",
      "         -0.3783, -0.2094,  0.2640,  0.5699, -0.1173, -0.1478,  0.1039,  0.1238,\n",
      "         -0.2431,  0.0263,  0.2342,  0.5383]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ReLU\n",
    "비선형 활성화는 모델의 입력과 출력 사이에 복잡한 매핑을 만드는 것입니다. 선형 변환 후에 적용되어 *비선형성* 을 도입하여 신경망이 다양한 현상을 하습하도록 돕습니다.\n",
    "\n",
    "이 모델에서는 선형 레이어 간에 `nn.ReLU`를 사용 하지만 모델에 비선형성을 도입하기 위한 다른 활성화가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2474,  0.4527, -0.6020, -0.1155, -0.3309, -0.1676, -0.2441, -0.9399,\n",
      "         -0.6062, -0.2157,  0.5055,  0.6485, -0.1223, -0.1734,  0.2810,  0.0141,\n",
      "         -0.0927, -0.2233,  0.3967,  0.2691],\n",
      "        [ 0.3556,  0.5093, -0.4995, -0.2223, -0.0264, -0.4979, -0.0013, -0.4729,\n",
      "         -0.2557, -0.3703,  0.1600,  0.9676, -0.0381, -0.0076, -0.1474, -0.1116,\n",
      "         -0.1684,  0.2044,  0.5976,  0.1881],\n",
      "        [ 0.3988,  0.5375, -0.9643, -0.2940, -0.1489, -0.3664, -0.0280, -0.5056,\n",
      "         -0.3783, -0.2094,  0.2640,  0.5699, -0.1173, -0.1478,  0.1039,  0.1238,\n",
      "         -0.2431,  0.0263,  0.2342,  0.5383]], grad_fn=<AddmmBackward0>)\n",
      "After ReLU: tensor([[0.2474, 0.4527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5055, 0.6485, 0.0000, 0.0000, 0.2810, 0.0141, 0.0000, 0.0000,\n",
      "         0.3967, 0.2691],\n",
      "        [0.3556, 0.5093, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1600, 0.9676, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2044,\n",
      "         0.5976, 0.1881],\n",
      "        [0.3988, 0.5375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2640, 0.5699, 0.0000, 0.0000, 0.1039, 0.1238, 0.0000, 0.0263,\n",
      "         0.2342, 0.5383]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Before ReLU: {0}'.format(hidden1))\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print('After ReLU: {0}'.format(hidden1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "\n",
    "`nn.Sequential`은 순서가 지정된 모듈 컨테이너입니다. 데이터는 정의된 것과 동일한 순서로 모든 모듈을 통해 전달됩니다. 순차 컨테이너를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2127,  0.0380,  0.1381, -0.2528, -0.1236,  0.0436,  0.0030,  0.2921,\n",
      "          0.0024, -0.1352],\n",
      "        [ 0.2580, -0.0098,  0.1397, -0.3172, -0.0773,  0.1411,  0.1899,  0.3030,\n",
      "          0.0407, -0.0591],\n",
      "        [ 0.2247,  0.0909,  0.1360, -0.2133,  0.0224,  0.1266,  0.0388,  0.1836,\n",
      "          0.0056, -0.0943]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Softmax\n",
    "신경망의 마지막 선형 계층은 nn.Softmax 모듈로 전달되는 [-infty, infty]의 원시 값인 logits을 반환합니다. 로짓은 각 클래스에 대한 모델의 예측 확률을 나타내는 값[0,1]로 조정됩니다. 매개변수는 값의 합이 1이 되어야 하는 차원을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1196, 0.1004, 0.1110, 0.0751, 0.0854, 0.1010, 0.0969, 0.1294, 0.0969,\n",
      "         0.0844],\n",
      "        [0.1200, 0.0918, 0.1066, 0.0675, 0.0858, 0.1068, 0.1121, 0.1255, 0.0966,\n",
      "         0.0874],\n",
      "        [0.1179, 0.1032, 0.1079, 0.0761, 0.0963, 0.1069, 0.0979, 0.1132, 0.0947,\n",
      "         0.0857]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc4cb84d376c55cbcefbb2577a30e93afba84ecdcb02495984b572f007c87af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
