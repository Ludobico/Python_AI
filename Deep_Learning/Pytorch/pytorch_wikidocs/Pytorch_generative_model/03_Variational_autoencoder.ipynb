{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변형 오토인코더\n",
    "변형 오토인코더(Variational autoencoder)를 좀 더 쉽게 이해할 수 있게 오토인코더와 비교하면서 설명하겠습니다. 오토인코더는 다음 그림과 같이  \n",
    "`입력(숫자 2)` -> `인코더` -> `압축(차원축소)` -> `디코더` -> `출력(숫자2)`  \n",
    "이 나오게 하는 방법입니다.  \n",
    "![](../Static/695_2.jpg)  \n",
    "\n",
    "오토인코더는 차원을 줄이는 것이 목표이기 때문에 새롭게 생성된 데이터의 확률 분포에는 관심이 없습니다.\n",
    "\n",
    "반면 변형 오토인코더는 표준편차와 평균을 이용하여 확률 분포를 만들고, 거기에서 샘플링하여 디코더를 통과시킨 후 새로운 데이터를 만들어 냅니다. 즉, 변형 오토인코더는 `입력 데이터와 조금 다른 출력 데이터`를 만들어 내는데, 이때 z라는 가우시안 분포를 이용합니다.(z를 잠재벡터(latent vector)라고 합니다.) 중요한 특성의 파라미터를 담고 있는 z 분포에서 벡터를 랜덤하게 샘플링하고 이 분포의 오차를 이용하여 입력 데이터와 유사한 다양한 데이터를 만들어 내는 것이 변형 오토인코더입니다.  \n",
    "\n",
    "![](../Static/696_1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오토인코더는 데이터 벡터에 대한 차원을 축소하여 실제 이미지와 동일한 이미지를 출력하는 것이 목적이었다면, 변형 오토인코더는 데이터가 만들어지는 확률 분포를 찾아 비슷한 데이터를 생성하는 것이 목적입니다.\n",
    "\n",
    "변형 오토인코더에서 인코더와 디코더에 대한 네트워크는 다음 그림과 같습니다.  \n",
    "![](../Static/696_2.jpg)  \n",
    "\n",
    "* qϕ(z|x) : x를 입력받아 잠재 벡터 z와 대응되는 평균과 분산을 구하는 네트워크로 인코더 네트워크를 의미합니다.\n",
    "\n",
    "* pθ(x|z) : z를 입력받아 x와 대응되는 평균과 분산을 구하는 네트워크로 디코더 네트워크를 의미합니다.\n",
    "\n",
    "그럼 인코더 네트워크부터 자세히 살펴보겠습니다.  \n",
    "![](../Static/697_1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 예제에서는 텐서보드에서 에포크 진행에 따른 오차를 확인할 예정입니다. 따라서 다음 명령으로 텐서보드를 설치합니다. 텐서보드 사용을 위해 `텐서보드 엑스(tensorboardX)` 라이브러리를 설치해야 합니다.\n",
    "\n",
    "pip install rensorboardX\n",
    "\n",
    "텐서보드 엑스는 학습 과정을 시각적으로 확인하고자 할 때 사용하는 도구입니다.\n",
    "\n",
    "변형 오토인코더에서도 오토인코더에서와 마찬가지로 MNIST 데이터셋을 이용합니다. 먼저 필요한 라이브러리를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터셋을 내려받은 후 이미지를 텐서로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./MNIST_DATA/',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./MNIST_DATA/',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=100, shuffle=True, num_workers=4, pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=100, shuffle=False, num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 네트워크를 생성합니다. 네트워크는 오토인코더처럼 인코더와 디코더로 구성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.input1(x))\n",
    "        h_ = self.LeakyReLU(self.input2(h_))\n",
    "        mean = self.mean(h_)\n",
    "        log_var = self.var(h_)\n",
    "        return mean, log_var # 인코더 네트워크에서 평균과 분산을 반환\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더 역할은 데이터(x)가 주어졌을 때 디코더가 원래 데이터로 잘 복원할 수 있는 이상적인 확률 분포 p(z|x)를 찾는 것입니다. 변형 오토인코더에서는 이상적인 확률 분포를 찾는데 변분추론을 사용합니다.\n",
    "\n",
    "이번에는 디코더 네트워크를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.hidden1(x))\n",
    "        h = self.LeakyReLU(self.hidden2(h))\n",
    "        x_hat = torch.sigmoid(self.output(h))\n",
    "        return x_hat # 디코더 결과는 시그모이드를 통과했으므로, 0~1 값을 갖습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더는 추출한 샘플을 입력으로 받아 다시 원본으로 재구축(재생성)하는 역할을 합니다.\n",
    "\n",
    "이제 평균과 표준편차가 주어졌을 때 잠재 벡터 z를 만들기 위해 `reparameterization()` 이라는 이름으로 함수를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device=device)\n",
    "        z = mean + var * epsilon # z값 구하기\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        x_hat = self.Decoder(z)\n",
    "        return x_hat, mean, log_var # 디코더의 결과와 평균, 표준편차(log를 취한 표준편차)를 반환"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `reparameterization()` 함수는 z 벡터를 샘플링하기 위한 용도입니다. z는 가우시안 분포라고 가정했기 때문에 인코더에서 받아 온 평균과 표준편차를 이용하여 z를 생성합니다. 그리고 z벡터를 디코더에 다시 통과시켜서 입력과 동일한 데이터(x_hat)를 만들어 내는 작업을 합니다.\n",
    "\n",
    "* 인코더에서 받아 온 평균과 표준편차를 이용하지만 표준편차는 값을 그대로 사용하지 않습니다. 값이 음수가 되지 않도록 로그(log)를 취하는데, 다음과 같은 방식을 취합니다.  \n",
    "![](../Static/fn2-88.jpg)  \n",
    "따라서 변수 이름도 `var`에서 `log_var`로 변경했습니다.\n",
    "\n",
    "필요한 모델의 네트워크(인코더와 디코더) 객체를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차를 계산하기 위한 손실 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(\n",
    "    x_hat, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss, KLD\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `loss_function` 오차를 구하는 함수입니다. 변분추론으로  p(z|x)와 q(z) 사이의 `쿨백-라이블러 발산(KLD)`을 계산하고, KLD가 줄어드는 쪽으로 q(z)를 조금씩 업데이트합니다. 즉, 변형 오토인코더에서 손실 함수가 쿨백-라이블러 발산이 됩니다. 즉, 손실 함수에서 반환되는 값을 수식처럼 모두 더하여 사용하는 것이 최종 손실 함수가 됩니다.\n",
    "\n",
    "이제 모델 학습에 필요한 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_loc = 'scalar/' # 텐서보드에서 사용할 경로\n",
    "writer = SummaryWriter(saved_loc)\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "def train(epoch, model, train_loader, optimizer):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "        loss = BCE + KLD\n",
    "        writer.add_scalar(\"Train/Reconstruction Error\", BCE.item(), batch_idx + epoch *\n",
    "                          (len(train_loader.dataset)/batch_size))\n",
    "        writer.add_scalar(\"Train/KL-Divergence\", KLD.item(), batch_idx + epoch *\n",
    "                          (len(train_loader.dataset)/batch_size))\n",
    "        writer.add_scalar(\"Train/Total Loss\", loss.item(), batch_idx + epoch *\n",
    "                          (len(train_loader.dataset)/batch_size))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                  epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                  100. * batch_idx / len(train_loader),\n",
    "                  loss.item() / len(x)))\n",
    "            \n",
    "    print(\"======> Epoch: {} Average loss: {:.4f}\".format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, _) in enumerate(test_loader):\n",
    "            x = x.view(batch_size, x_dim)\n",
    "            x = x.to(device)\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "            loss = BCE + KLD\n",
    "\n",
    "            writer.add_scalar(\"Test/Reconstruction Error\", BCE.item(), batch_idx +\n",
    "                              epoch * (len(test_loader.dataset)/batch_size))\n",
    "            writer.add_scalar(\"Test/KL-Divergence\", KLD.item(), batch_idx + epoch *\n",
    "                              (len(test_loader.dataset)/batch_size))\n",
    "            writer.add_scalar(\"Test/Total Loss\", loss.item(), batch_idx + epoch *\n",
    "                              (len(test_loader.dataset)/batch_size))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                n = min(x.size(0), 8)\n",
    "                comparison = torch.cat(\n",
    "                    [x[:n], x_hat.view(batch_size, x_dim)[:n]])\n",
    "                grid = torchvision.utils.make_grid(comparison.cpu())\n",
    "                writer.add_image(\n",
    "                    \"Test image - Above: Real data, below: reconstruction data\", grid, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\t Loss: 544.445273\n",
      "Train Epoch: 0 [10000/60000 (17%)]\t Loss: 191.841836\n",
      "Train Epoch: 0 [20000/60000 (33%)]\t Loss: 178.854609\n",
      "Train Epoch: 0 [30000/60000 (50%)]\t Loss: 161.586797\n",
      "Train Epoch: 0 [40000/60000 (67%)]\t Loss: 147.413799\n",
      "Train Epoch: 0 [50000/60000 (83%)]\t Loss: 145.488457\n",
      "======> Epoch: 0 Average loss: 173.3627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:05<02:47,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\t Loss: 142.705996\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t Loss: 137.817070\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t Loss: 131.798984\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t Loss: 128.401875\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t Loss: 125.988242\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t Loss: 125.065576\n",
      "======> Epoch: 1 Average loss: 128.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:11<02:44,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\t Loss: 115.598828\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t Loss: 122.667578\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t Loss: 113.275068\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t Loss: 115.082354\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t Loss: 118.797383\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t Loss: 118.524473\n",
      "======> Epoch: 2 Average loss: 116.6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:17<02:39,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\t Loss: 114.358828\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t Loss: 113.378789\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t Loss: 114.837002\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t Loss: 108.953711\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t Loss: 114.066016\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t Loss: 110.708359\n",
      "======> Epoch: 3 Average loss: 112.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:24<02:39,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\t Loss: 108.001680\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t Loss: 109.257012\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t Loss: 108.184766\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t Loss: 117.802373\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t Loss: 104.079063\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t Loss: 107.298262\n",
      "======> Epoch: 4 Average loss: 109.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:31<02:42,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\t Loss: 102.931641\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t Loss: 109.148750\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t Loss: 104.243281\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t Loss: 113.460781\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t Loss: 106.778516\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t Loss: 110.352803\n",
      "======> Epoch: 5 Average loss: 108.2942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:38<02:40,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\t Loss: 107.660078\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t Loss: 104.575469\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t Loss: 108.501895\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t Loss: 108.682773\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t Loss: 105.679541\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t Loss: 111.163125\n",
      "======> Epoch: 6 Average loss: 107.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:45<02:34,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\t Loss: 105.835547\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t Loss: 105.432715\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t Loss: 106.257695\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t Loss: 106.381738\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t Loss: 105.436230\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t Loss: 103.834551\n",
      "======> Epoch: 7 Average loss: 106.1421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:52<02:30,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\t Loss: 111.599902\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t Loss: 102.340801\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t Loss: 102.468506\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t Loss: 105.494883\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t Loss: 103.981309\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t Loss: 107.523691\n",
      "======> Epoch: 8 Average loss: 105.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:59<02:24,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\t Loss: 105.032734\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t Loss: 109.598047\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t Loss: 106.352373\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t Loss: 108.938340\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t Loss: 109.340967\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t Loss: 102.856953\n",
      "======> Epoch: 9 Average loss: 104.7786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [01:06<02:19,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\t Loss: 104.032578\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t Loss: 102.570820\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t Loss: 100.445342\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t Loss: 102.032686\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t Loss: 105.200381\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t Loss: 106.459355\n",
      "======> Epoch: 10 Average loss: 104.1607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [01:14<02:18,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\t Loss: 102.224980\n",
      "Train Epoch: 11 [10000/60000 (17%)]\t Loss: 104.727148\n",
      "Train Epoch: 11 [20000/60000 (33%)]\t Loss: 103.391523\n",
      "Train Epoch: 11 [30000/60000 (50%)]\t Loss: 103.236553\n",
      "Train Epoch: 11 [40000/60000 (67%)]\t Loss: 104.992930\n",
      "Train Epoch: 11 [50000/60000 (83%)]\t Loss: 102.446416\n",
      "======> Epoch: 11 Average loss: 103.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [01:22<02:17,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\t Loss: 101.700166\n",
      "Train Epoch: 12 [10000/60000 (17%)]\t Loss: 105.804375\n",
      "Train Epoch: 12 [20000/60000 (33%)]\t Loss: 103.051260\n",
      "Train Epoch: 12 [30000/60000 (50%)]\t Loss: 101.129355\n",
      "Train Epoch: 12 [40000/60000 (67%)]\t Loss: 105.013037\n",
      "Train Epoch: 12 [50000/60000 (83%)]\t Loss: 104.820723\n",
      "======> Epoch: 12 Average loss: 103.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [01:31<02:17,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\t Loss: 106.955098\n",
      "Train Epoch: 13 [10000/60000 (17%)]\t Loss: 100.887793\n",
      "Train Epoch: 13 [20000/60000 (33%)]\t Loss: 101.258672\n",
      "Train Epoch: 13 [30000/60000 (50%)]\t Loss: 104.975000\n",
      "Train Epoch: 13 [40000/60000 (67%)]\t Loss: 103.370391\n",
      "Train Epoch: 13 [50000/60000 (83%)]\t Loss: 107.847295\n",
      "======> Epoch: 13 Average loss: 102.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [01:40<02:12,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\t Loss: 97.537803\n",
      "Train Epoch: 14 [10000/60000 (17%)]\t Loss: 101.229062\n",
      "Train Epoch: 14 [20000/60000 (33%)]\t Loss: 105.658340\n",
      "Train Epoch: 14 [30000/60000 (50%)]\t Loss: 106.813027\n",
      "Train Epoch: 14 [40000/60000 (67%)]\t Loss: 102.824277\n",
      "Train Epoch: 14 [50000/60000 (83%)]\t Loss: 99.236191\n",
      "======> Epoch: 14 Average loss: 102.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [01:49<02:05,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\t Loss: 98.034023\n",
      "Train Epoch: 15 [10000/60000 (17%)]\t Loss: 103.976191\n",
      "Train Epoch: 15 [20000/60000 (33%)]\t Loss: 105.469785\n",
      "Train Epoch: 15 [30000/60000 (50%)]\t Loss: 104.643008\n",
      "Train Epoch: 15 [40000/60000 (67%)]\t Loss: 102.554756\n",
      "Train Epoch: 15 [50000/60000 (83%)]\t Loss: 100.067656\n",
      "======> Epoch: 15 Average loss: 102.2868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [01:58<02:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\t Loss: 100.829121\n",
      "Train Epoch: 16 [10000/60000 (17%)]\t Loss: 102.628008\n",
      "Train Epoch: 16 [20000/60000 (33%)]\t Loss: 104.638818\n",
      "Train Epoch: 16 [30000/60000 (50%)]\t Loss: 94.765762\n",
      "Train Epoch: 16 [40000/60000 (67%)]\t Loss: 103.440137\n",
      "Train Epoch: 16 [50000/60000 (83%)]\t Loss: 99.085713\n",
      "======> Epoch: 16 Average loss: 102.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [02:07<01:53,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\t Loss: 103.042012\n",
      "Train Epoch: 17 [10000/60000 (17%)]\t Loss: 100.954980\n",
      "Train Epoch: 17 [20000/60000 (33%)]\t Loss: 102.094180\n",
      "Train Epoch: 17 [30000/60000 (50%)]\t Loss: 99.854092\n",
      "Train Epoch: 17 [40000/60000 (67%)]\t Loss: 97.044375\n",
      "Train Epoch: 17 [50000/60000 (83%)]\t Loss: 102.189883\n",
      "======> Epoch: 17 Average loss: 101.8102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [02:16<01:47,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\t Loss: 103.111611\n",
      "Train Epoch: 18 [10000/60000 (17%)]\t Loss: 102.572012\n",
      "Train Epoch: 18 [20000/60000 (33%)]\t Loss: 105.445957\n",
      "Train Epoch: 18 [30000/60000 (50%)]\t Loss: 101.361641\n",
      "Train Epoch: 18 [40000/60000 (67%)]\t Loss: 101.874209\n",
      "Train Epoch: 18 [50000/60000 (83%)]\t Loss: 99.598281\n",
      "======> Epoch: 18 Average loss: 101.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [02:27<01:43,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\t Loss: 104.396914\n",
      "Train Epoch: 19 [10000/60000 (17%)]\t Loss: 104.142207\n",
      "Train Epoch: 19 [20000/60000 (33%)]\t Loss: 98.749121\n",
      "Train Epoch: 19 [30000/60000 (50%)]\t Loss: 97.220098\n",
      "Train Epoch: 19 [40000/60000 (67%)]\t Loss: 102.527598\n",
      "Train Epoch: 19 [50000/60000 (83%)]\t Loss: 100.989355\n",
      "======> Epoch: 19 Average loss: 101.4884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [02:37<01:37,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\t Loss: 100.845625\n",
      "Train Epoch: 20 [10000/60000 (17%)]\t Loss: 100.785820\n",
      "Train Epoch: 20 [20000/60000 (33%)]\t Loss: 103.622559\n",
      "Train Epoch: 20 [30000/60000 (50%)]\t Loss: 101.952344\n",
      "Train Epoch: 20 [40000/60000 (67%)]\t Loss: 99.310713\n",
      "Train Epoch: 20 [50000/60000 (83%)]\t Loss: 99.161455\n",
      "======> Epoch: 20 Average loss: 101.2565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [02:49<01:32, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\t Loss: 102.702988\n",
      "Train Epoch: 21 [10000/60000 (17%)]\t Loss: 97.627627\n",
      "Train Epoch: 21 [20000/60000 (33%)]\t Loss: 101.022920\n",
      "Train Epoch: 21 [30000/60000 (50%)]\t Loss: 100.022295\n",
      "Train Epoch: 21 [40000/60000 (67%)]\t Loss: 102.374707\n",
      "Train Epoch: 21 [50000/60000 (83%)]\t Loss: 102.559160\n",
      "======> Epoch: 21 Average loss: 101.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [03:02<01:28, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\t Loss: 100.604883\n",
      "Train Epoch: 22 [10000/60000 (17%)]\t Loss: 103.616260\n",
      "Train Epoch: 22 [20000/60000 (33%)]\t Loss: 100.198115\n",
      "Train Epoch: 22 [30000/60000 (50%)]\t Loss: 98.316621\n",
      "Train Epoch: 22 [40000/60000 (67%)]\t Loss: 95.590937\n",
      "Train Epoch: 22 [50000/60000 (83%)]\t Loss: 96.522266\n",
      "======> Epoch: 22 Average loss: 100.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [03:16<01:24, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\t Loss: 100.110195\n",
      "Train Epoch: 23 [10000/60000 (17%)]\t Loss: 104.652246\n",
      "Train Epoch: 23 [20000/60000 (33%)]\t Loss: 101.872754\n",
      "Train Epoch: 23 [30000/60000 (50%)]\t Loss: 100.772314\n",
      "Train Epoch: 23 [40000/60000 (67%)]\t Loss: 101.700625\n",
      "Train Epoch: 23 [50000/60000 (83%)]\t Loss: 102.562559\n",
      "======> Epoch: 23 Average loss: 100.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [03:33<01:21, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\t Loss: 102.914834\n",
      "Train Epoch: 24 [10000/60000 (17%)]\t Loss: 102.796660\n",
      "Train Epoch: 24 [20000/60000 (33%)]\t Loss: 96.432637\n",
      "Train Epoch: 24 [30000/60000 (50%)]\t Loss: 101.040469\n",
      "Train Epoch: 24 [40000/60000 (67%)]\t Loss: 101.767441\n",
      "Train Epoch: 24 [50000/60000 (83%)]\t Loss: 101.269072\n",
      "======> Epoch: 24 Average loss: 100.7663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [03:48<01:09, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\t Loss: 101.833125\n",
      "Train Epoch: 25 [10000/60000 (17%)]\t Loss: 103.791904\n",
      "Train Epoch: 25 [20000/60000 (33%)]\t Loss: 101.912148\n",
      "Train Epoch: 25 [30000/60000 (50%)]\t Loss: 100.489102\n",
      "Train Epoch: 25 [40000/60000 (67%)]\t Loss: 103.101172\n",
      "Train Epoch: 25 [50000/60000 (83%)]\t Loss: 97.818984\n",
      "======> Epoch: 25 Average loss: 100.6637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [03:59<00:52, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\t Loss: 103.680234\n",
      "Train Epoch: 26 [10000/60000 (17%)]\t Loss: 95.916855\n",
      "Train Epoch: 26 [20000/60000 (33%)]\t Loss: 105.617266\n",
      "Train Epoch: 26 [30000/60000 (50%)]\t Loss: 100.566191\n",
      "Train Epoch: 26 [40000/60000 (67%)]\t Loss: 99.304873\n",
      "Train Epoch: 26 [50000/60000 (83%)]\t Loss: 102.819775\n",
      "======> Epoch: 26 Average loss: 100.4655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [04:09<00:36, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\t Loss: 99.935742\n",
      "Train Epoch: 27 [10000/60000 (17%)]\t Loss: 100.342793\n",
      "Train Epoch: 27 [20000/60000 (33%)]\t Loss: 99.475176\n",
      "Train Epoch: 27 [30000/60000 (50%)]\t Loss: 97.414219\n",
      "Train Epoch: 27 [40000/60000 (67%)]\t Loss: 100.929502\n",
      "Train Epoch: 27 [50000/60000 (83%)]\t Loss: 100.769287\n",
      "======> Epoch: 27 Average loss: 100.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [04:18<00:22, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\t Loss: 100.205801\n",
      "Train Epoch: 28 [10000/60000 (17%)]\t Loss: 104.510400\n",
      "Train Epoch: 28 [20000/60000 (33%)]\t Loss: 103.579268\n",
      "Train Epoch: 28 [30000/60000 (50%)]\t Loss: 103.277480\n",
      "Train Epoch: 28 [40000/60000 (67%)]\t Loss: 100.476182\n",
      "Train Epoch: 28 [50000/60000 (83%)]\t Loss: 102.305391\n",
      "======> Epoch: 28 Average loss: 100.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [04:27<00:10, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\t Loss: 104.560234\n",
      "Train Epoch: 29 [10000/60000 (17%)]\t Loss: 100.274395\n",
      "Train Epoch: 29 [20000/60000 (33%)]\t Loss: 102.401270\n",
      "Train Epoch: 29 [30000/60000 (50%)]\t Loss: 97.119531\n",
      "Train Epoch: 29 [40000/60000 (67%)]\t Loss: 97.905664\n",
      "Train Epoch: 29 [50000/60000 (83%)]\t Loss: 100.205576\n",
      "======> Epoch: 29 Average loss: 100.2509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [04:36<00:00,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "for epoch in tqdm(range(0, epochs)):\n",
    "    train(epoch, model, train_loader, optimizer)\n",
    "    test(epoch, model, test_loader)\n",
    "    print(\"\\n\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "969136afa1ab1e40cee0417c89a6b891bc5152d071e81b6238bc15b76e770f08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
