{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝의 학습 방법\n",
    "* 순전파(Forward Propagation)\n",
    "    * 활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고나면 입력값은 입력층,은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며 출력층으로 향한다. 그리고 출력층에서 모든 연산을 마친 예측값이 나오게 된다.\n",
    "    * 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 순전파라고 한다.\n",
    "\n",
    "* 오차역전파(Back Propagation)\n",
    "    * 트레이닝 데이터에 대하여 피드포워드를 수행한 후에 손실 함수를 계산하고, 손실 함수가 최소가 아니면 수치 미분을 통해서 가중치 w, 바이러스 b를 업데이트한 후에 다시 피드포워드를 반복 수행한다. 이러한 수치미분으로 가중치와 바이어스를 업데이트할 경우에 많은 시간이 소요된다는 치명적인 단점이 있다. 이러한 단점을 극복하고 딥러닝 성능을 개선시킨 알고리즘이 바로 오차역전파이다.\n",
    "    * 오차역전파는 수치 미분을 사용하지 않고 행렬(Matrix)로 표현되는 수학공식으로 계산되기 때문에 빠른 계산이 가능하다는 장점을 가지고 있다.\n",
    "\n",
    "* 손실 함수(Loss function)\n",
    "    * 실제값과 예측값의 차이를 수치화해주는 함수를 말한다.\n",
    "    * 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아진다.\n",
    "    * 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b를 찾아가는 것이 딥러닝의 학습 과정이므로 손실 함수의 선정은 매우 중요하다.\n",
    "    * 회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용한다.\n",
    "        * MSE(Mean Squared Error)\n",
    "            - 오차 제곱 평균을 의미\n",
    "            - 회귀모델에서 연속형 변수를 예측할 때 사용\n",
    "        * 크로스 엔트로피(Cross-Entropy)\n",
    "            - 분류 모델에서 실제 값과 예측값의 차이(dissimilarity)를 계산하는데 사용\n",
    "            - 이진 분류: binary_crossentropy를 사용, 다중 분류: categorical_crossentropy 사용\n",
    "* 옵티마이저(Optimizer)\n",
    "    * 실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다.\n",
    "    * 배치 경사 하강법(Batch Gradient Descent)\n",
    "        - 가장 기본적인 경사 하강법으로 오차(loss)를 구할 때 전체 데이터를 고려한다.\n",
    "        - 머신 러닝에선는 1번의 훈련 횟수를 1 에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행한다.\n",
    "        - 배치 경사 하강법은 전체 데이터를 고려해서 학습하므로 에포크당 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있으나 글로벌 미니멈을 찾을 수 있다는 장점이 있다.\n",
    "        - 사용예) model.fit(x_train, y_train, batch_size=len(trainx))\n",
    "    * 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
    "        - 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법이다.\n",
    "    * 미니 배치 경사 하강법(Mini-Batch Gradient Descent)\n",
    "        - 전체 데이터도 아니고, 1개의 데이터도 아니고 정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법을 말한다.\n",
    "    * 모멘텀(Momentum)\n",
    "        - 관성이라는 물리학의 법칙을 응용한 방법으로 모멘텀 SGD는 경사 하강법에 관성을 더 해준다.\n",
    "    * 아다그라드(Adagrad)\n",
    "    * 알엠에스프롬(RMSprop)\n",
    "    * 아담(Adam: Adaptive Moment Estimation)\n",
    "        - 알엠에스프롭과 모멘텀 두 가지를 합친 방법으로, 방향과 학습 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90ac13e2ffd2fb0a306c90461e8e391bceeb8ea777ddbefad745a048bb7968fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
