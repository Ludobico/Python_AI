{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b5217a",
   "metadata": {},
   "source": [
    "# 머신러닝 알고리즘 분류\n",
    "* 회귀 분석(지도학습)\n",
    "    * Linear Regression\n",
    "\n",
    "* 분류 분석(지도학습)\n",
    "    * Logistic Regression\n",
    "    * K-Nearest Neighbor\n",
    "    * Decision Tree\n",
    "    * Random Forest\n",
    "    * Naive Bayes Classifier\n",
    "    * SVM\n",
    "\n",
    "* 군집분석(clustering)(비지도 학습)\n",
    "    * K-Means Clustering\n",
    "    * Hierarchical Clustering\n",
    "    * Density Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc966b",
   "metadata": {},
   "source": [
    "## 알고리즘 설명\n",
    "* 선형회귀모델은 2차원 상의 일직선이므로 다음과 같은 다항함수로 표현할 수 있다.\n",
    "####  y = wx + b\n",
    "\n",
    "* 모델에 특정한 값을 입력했을 때 얻어지는 결과를 y^ 라고한다. y^는 모델이 만들어낸 예측값이다.\n",
    "#### y^ = y\n",
    "\n",
    "### Linear Regression 종류\n",
    "* 단순 선형 회귀\n",
    "    * 단순 선형 회귀는 독립변수가 1개인 선형 회귀를 말한다. y = wx + b\n",
    "\n",
    "* 다중 선형 회귀\n",
    "    * feature가 2개 이상인 선형 회귀를 말한다. y = w1x1 + w2x2 + ... +b\n",
    "\n",
    "### Cost function\n",
    "* 실제값 y와 예측값 y^의 차이를 오차(error)라고 한다.\n",
    "* 기계학습은 실제값과 예측값의 오차(error)를 최소화하는 w와 b를 찾아가는 과정이다.\n",
    "* w, b와 오차의 관계를 함수로 나타낸 것을 비용함수(cost function) 또는 손실함수(Loss function)이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46b972",
   "metadata": {},
   "source": [
    "### 평균제곱오차(MSE, Mean Squared Errors)\n",
    "* 오차를 전체적으로 줄여야 예측의 정확도가 높아진다.\n",
    "* MSE = n/1 * sigma(n)(i = 1) (yi = y^i)\n",
    "\n",
    "### Optimizer\n",
    "* 비용함수를 최소화하는 w와 b를 구하는 최적화 알고리즘을 옵티마이저라 부른다.\n",
    "    * 경사하강법(Gradient Descent)\n",
    "        * 가장 기본적인 옵티마이저이다.\n",
    "        * 비용함수의 기울기가 작아지는 방향으로 w와 b를 업데이트 한다.\n",
    "        * 비용함수를 그래프로 그리면 x ** 2 그래프가 된다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55165c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wb [[0.45471849]] [0.80007115]\n",
      "Initial loss value =  3.964727708594824 Initial w = [[0.45471849]] \n",
      " ,b =  [0.80007115]\n",
      "step =  0 loss value =  2.3328831126996543 W = [[0.58667616]] b = [0.82886916]\n",
      "step =  300 loss value =  0.00013768789406656405 W = [[1.00761982]] b = [0.97249677]\n",
      "step =  600 loss value =  1.7480000782876963e-05 W = [[1.00271499]] b = [0.99020044]\n",
      "step =  900 loss value =  2.2191524493906474e-06 W = [[1.00096736]] b = [0.99650836]\n",
      "step =  1200 loss value =  2.8172982683519627e-07 W = [[1.00034468]] b = [0.99875591]\n",
      "step =  1500 loss value =  3.576667089740561e-08 W = [[1.00012281]] b = [0.99955672]\n",
      "step =  1800 loss value =  4.540714632345658e-09 W = [[1.00004376]] b = [0.99984206]\n",
      "step =  2100 loss value =  5.764609580783928e-10 W = [[1.00001559]] b = [0.99994372]\n",
      "step =  2400 loss value =  7.318390673760429e-11 W = [[1.00000556]] b = [0.99997995]\n",
      "step =  2700 loss value =  9.290974748715367e-12 W = [[1.00000198]] b = [0.99999286]\n",
      "step =  3000 loss value =  1.1795245111192574e-12 W = [[1.00000071]] b = [0.99999745]\n",
      "step =  3300 loss value =  1.497451137931352e-13 W = [[1.00000025]] b = [0.99999909]\n",
      "step =  3600 loss value =  1.901071062823266e-14 W = [[1.00000009]] b = [0.99999968]\n",
      "step =  3900 loss value =  2.413481879217361e-15 W = [[1.00000003]] b = [0.99999988]\n",
      "step =  4200 loss value =  3.0640069272420806e-16 W = [[1.00000001]] b = [0.99999996]\n",
      "step =  4500 loss value =  3.889873024363627e-17 W = [[1.]] b = [0.99999999]\n",
      "step =  4800 loss value =  4.938342149213425e-18 W = [[1.]] b = [0.99999999]\n",
      "step =  5100 loss value =  6.269413588139555e-19 W = [[1.]] b = [1.]\n",
      "step =  5400 loss value =  7.959252113420263e-20 W = [[1.]] b = [1.]\n",
      "step =  5700 loss value =  1.0104559970068915e-20 W = [[1.]] b = [1.]\n",
      "step =  6000 loss value =  1.2828234030396246e-21 W = [[1.]] b = [1.]\n"
     ]
    }
   ],
   "source": [
    "#경사하강법\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1) #5행 1열 2차원 데이터로 변환\n",
    "t_data = np.array([2,3,4,5,6]).reshape(5,1) # 레이블\n",
    "w = np.random.rand(1,1) # 기울기, 2차원 , 1미만의 난수생성\n",
    "b = np.random.rand(1) # bias, 1차원, 1미만의 난수생성\n",
    "\n",
    "def loss_func(x,t): #손실함수(MSE)\n",
    "    y = np.dot(x,w) + b #dot은 행렬연산자\n",
    "    return (np.sum((t - y)**2)) / (len(x))\n",
    "\n",
    "\n",
    "def numerial_derivative(f,x): #수치미분 함수\n",
    "    delta_x = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "learning_rate = 1e-2 #학습률 알파\n",
    "f = lambda x: loss_func(x_data, t_data)\n",
    "print('Initial loss value = ', loss_func(x_data, t_data), \"Initial w =\", w, '\\n',',b = ',b)\n",
    "for step in range(6001):\n",
    "    w -= learning_rate * numerial_derivative(f,w)\n",
    "    b -= learning_rate * numerial_derivative(f,b)\n",
    "    if (step % 300 == 0): #추가부분\n",
    "        print(\"step = \", step, \"loss value = \", loss_func(x_data, t_data), \"W =\", w, \"b =\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83efc3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(x):\n",
    "    y = np.dot(x,w) + b\n",
    "    return y\n",
    "\n",
    "predict(np.array([43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb58c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [5 6 7 8]] \n",
      "\n",
      "a.shape ==  (2, 4) \n",
      "\n",
      "(0, 0) current value =>  1\n",
      "(0, 1) current value =>  2\n",
      "(0, 2) current value =>  3\n",
      "(0, 3) current value =>  4\n",
      "(1, 0) current value =>  5\n",
      "(1, 1) current value =>  6\n",
      "(1, 2) current value =>  7\n",
      "(1, 3) current value =>  8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4], [5,6,7,8]])\n",
    "\n",
    "print(a, '\\n')\n",
    "print('a.shape == ', a.shape, '\\n')\n",
    "\n",
    "it = np.nditer(a, flags=['multi_index'], op_flags=['readwrite'])\n",
    "while not it.finished:\n",
    "    idx = it.multi_index\n",
    "    print(idx, 'current value => ', a[idx])\n",
    "    it.iternext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c42a5",
   "metadata": {},
   "source": [
    "# 입력변수가 2개 이상인 선형회귀 예제(다중 회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81217878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.37454012]\n",
      " [0.95071431]\n",
      " [0.73199394]] , W.shape =  (3, 1) , b =  [0.59865848] , b.shape =  (1,)\n",
      "Initial loss value =  18.912018958218844\n",
      "step =  0 loss value =  11.964774630423882\n",
      "step =  3000 loss value =  4.418931926580983\n",
      "step =  6000 loss value =  3.8705650947081622\n",
      "step =  9000 loss value =  3.718669962755764\n",
      "step =  12000 loss value =  3.667505157681574\n",
      "step =  15000 loss value =  3.6490087100776845\n",
      "step =  18000 loss value =  3.642106732530343\n",
      "step =  21000 loss value =  3.639430652979643\n",
      "step =  24000 loss value =  3.638306266857524\n",
      "step =  27000 loss value =  3.6377530185756117\n",
      "step =  30000 loss value =  3.637410373246306\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "loaded_data = np.loadtxt('./data1.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = loaded_data[:,0:-1]\n",
    "t_data = loaded_data[:,[-1]]\n",
    "\n",
    "np.random.seed(42)\n",
    "w = np.random.rand(3,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", w, \", W.shape = \", w.shape, \", b = \", b, \", b.shape = \", b.shape)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "f = lambda x: loss_func(x_data,t_data)\n",
    "print(\"Initial loss value = \", loss_func(x_data, t_data) )\n",
    "for step in range(30001):\n",
    "    w -= learning_rate * numerial_derivative(f,w)\n",
    "    b -= learning_rate * numerial_derivative(f,b)\n",
    "    if(step % 3000 == 0):\n",
    "        print(\"step = \", step, \"loss value = \", loss_func(x_data, t_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c17297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75., 152.],\n",
       "       [ 93.,  88.,  93., 185.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 96.,  98., 100., 196.],\n",
       "       [ 73.,  66.,  70., 142.],\n",
       "       [ 53.,  46.,  55., 101.],\n",
       "       [ 69.,  74.,  77., 149.],\n",
       "       [ 47.,  56.,  60., 115.],\n",
       "       [ 87.,  79.,  90., 175.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302a9483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179.13680055])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100, 98, 81])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4299e",
   "metadata": {},
   "source": [
    "# Scikit-learn으로 다중회귀 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "150a6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.6844267283527141\n",
      "계수(w값):  [-1.28322638e-01  2.95517751e-02  4.88590934e-02  2.77350326e+00\n",
      " -1.62388292e+01  4.36875476e+00 -9.24808158e-03 -1.40086668e+00\n",
      "  2.57761243e-01 -9.95694820e-03 -9.23122944e-01  1.31854199e-02\n",
      " -5.17639519e-01]\n",
      "절편:  29.83642016383914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') # 경고메세지 무시함\n",
    "\n",
    "data = load_boston()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler #minmax 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_minmax = scaler.transform(x_train)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #Standard 스케일링\n",
    "scaler_St = StandardScaler()\n",
    "scaler_St.fit(x_train)\n",
    "x_train_St = scaler_St.transform(x_train)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train) #모델 학습\n",
    "\n",
    "\n",
    "#모델 예측 및 평가\n",
    "score = model.score(x_test, y_test)\n",
    "print('정확도: ', score)\n",
    "\n",
    "coefficient = model.coef_  #가중치 값\n",
    "intercept = model.intercept_ #bias 값\n",
    "\n",
    "print('계수(w값): ',coefficient)\n",
    "print('절편: ',intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca08588",
   "metadata": {},
   "source": [
    "## 단순 회귀 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42799768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target_names  \n",
       "0       15.3  396.90   4.98          24.0  \n",
       "1       17.8  396.90   9.14          21.6  \n",
       "2       17.8  392.83   4.03          34.7  \n",
       "3       18.7  394.63   2.94          33.4  \n",
       "4       18.7  396.90   5.33          36.2  \n",
       "..       ...     ...    ...           ...  \n",
       "501     21.0  391.99   9.67          22.4  \n",
       "502     21.0  396.90   9.08          20.6  \n",
       "503     21.0  396.90   5.64          23.9  \n",
       "504     21.0  393.45   6.48          22.0  \n",
       "505     21.0  396.90   7.88          11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "df['target_names'] = data.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd03fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
